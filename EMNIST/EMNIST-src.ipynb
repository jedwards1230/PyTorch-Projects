{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbe100f9dd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "continue_train = False\n",
    "\n",
    "os.makedirs('results', exist_ok=True)\n",
    "model_path = 'results/model.pth'\n",
    "optimizer_path = 'results/optimizer.pth'\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "log_interval = 200\n",
    "\n",
    "writer = SummaryWriter('logs')\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.EMNIST(\n",
    "            '../datasets/',\n",
    "            split = 'balanced',\n",
    "            train = True,\n",
    "            download = True,\n",
    "            transform = torchvision.transforms.Compose([ \n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize(\n",
    "                    (0.1307,),(0.3081,))\n",
    "                ])),\n",
    "        batch_size = batch_size_train,\n",
    "        shuffle = True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True)\n",
    " \n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.EMNIST(\n",
    "            '../datasets/',\n",
    "            split = 'balanced',\n",
    "            train = False,\n",
    "            download = True,\n",
    "            transform = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize(\n",
    "                    (0.1307,),(0.3081,))\n",
    "                ])),\n",
    "        batch_size = batch_size_test,\n",
    "        shuffle = True,\n",
    "        num_workers = 4,\n",
    "        pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 50, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(50, 100, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(100*4*4, 100)\n",
    "        self.fc2 = nn.Linear(100, 47)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net()\n",
    "network.to(device)\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "if continue_train:\n",
    "    network_state_dict = torch.load(model_path)\n",
    "    network.load_state_dict(network_state_dict)\n",
    "\n",
    "    optimizer_state_dict = torch.load(optimizer_path)\n",
    "    optimizer.load_state_dict(optimizer_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 50, 24, 24]           1,300\n",
      "            Conv2d-2            [-1, 100, 8, 8]         125,100\n",
      "         Dropout2d-3            [-1, 100, 8, 8]               0\n",
      "            Linear-4                  [-1, 100]         160,100\n",
      "            Linear-5                   [-1, 47]           4,747\n",
      "================================================================\n",
      "Total params: 291,247\n",
      "Trainable params: 291,247\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.32\n",
      "Params size (MB): 1.11\n",
      "Estimated Total Size (MB): 1.43\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(network, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    running_loss = 0.0\n",
    "    network.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = network(inputs)\n",
    "        loss = F.nll_loss(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % log_interval == 0:\n",
    "            step = (i+1) * epoch + (epoch-1) *len(train_loader)\n",
    "            writer.add_scalar('Loss/train', loss.item(), step)\n",
    "            \n",
    "            j = i * len(inputs)\n",
    "            k = round(100. * i / len(train_loader))\n",
    "\n",
    "            print(f'Train Epoch: {epoch} [{j}/{len(train_loader.dataset)} ({k}%)]\\tLoss: {round(loss.item(), 4)}')\n",
    "\n",
    "    torch.save(network.state_dict(), model_path)\n",
    "    torch.save(optimizer.state_dict(), optimizer_path)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(test_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            output = network(inputs)\n",
    "            test_loss += F.nll_loss(output, labels, reduction='sum').item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(labels.data.view_as(pred)).sum()\n",
    "    \n",
    "            if i % log_interval == 0:\n",
    "                step = (i+1) * epoch + (epoch-1) *len(test_loader)\n",
    "                writer.add_scalar('Loss/test', test_loss / len(test_loader.dataset), step)\n",
    "                \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "        \n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    test_accuracy = int(test_accuracy)\n",
    "    writer.add_scalar('Accuracy/test', test_accuracy, epoch)\n",
    "    print(f'\\nTest set: Avg. loss: {round(test_loss, 4)}, Accuracy: {correct}/{len(test_loader.dataset)} ({round(test_accuracy)}%)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/112800 (0%)]\tLoss: 3.8952\n",
      "Train Epoch: 1 [12800/112800 (11%)]\tLoss: 3.6453\n",
      "Train Epoch: 1 [25600/112800 (23%)]\tLoss: 2.9548\n",
      "Train Epoch: 1 [38400/112800 (34%)]\tLoss: 2.0267\n",
      "Train Epoch: 1 [51200/112800 (45%)]\tLoss: 1.8598\n",
      "Train Epoch: 1 [64000/112800 (57%)]\tLoss: 1.8846\n",
      "Train Epoch: 1 [76800/112800 (68%)]\tLoss: 1.4342\n",
      "Train Epoch: 1 [89600/112800 (79%)]\tLoss: 1.521\n",
      "Train Epoch: 1 [102400/112800 (91%)]\tLoss: 1.2152\n",
      "\n",
      "Test set: Avg. loss: 0.903, Accuracy: 13746/18800 (73%)\n",
      "\n",
      "Train Epoch: 2 [0/112800 (0%)]\tLoss: 1.2001\n",
      "Train Epoch: 2 [12800/112800 (11%)]\tLoss: 1.2319\n",
      "Train Epoch: 2 [25600/112800 (23%)]\tLoss: 1.2074\n",
      "Train Epoch: 2 [38400/112800 (34%)]\tLoss: 1.2057\n",
      "Train Epoch: 2 [51200/112800 (45%)]\tLoss: 1.298\n",
      "Train Epoch: 2 [64000/112800 (57%)]\tLoss: 1.2954\n",
      "Train Epoch: 2 [76800/112800 (68%)]\tLoss: 1.2128\n",
      "Train Epoch: 2 [89600/112800 (79%)]\tLoss: 0.9344\n",
      "Train Epoch: 2 [102400/112800 (91%)]\tLoss: 0.912\n",
      "\n",
      "Test set: Avg. loss: 0.6523, Accuracy: 14897/18800 (79%)\n",
      "\n",
      "Train Epoch: 3 [0/112800 (0%)]\tLoss: 1.0405\n",
      "Train Epoch: 3 [12800/112800 (11%)]\tLoss: 0.9893\n",
      "Train Epoch: 3 [25600/112800 (23%)]\tLoss: 0.8905\n",
      "Train Epoch: 3 [38400/112800 (34%)]\tLoss: 1.0492\n",
      "Train Epoch: 3 [51200/112800 (45%)]\tLoss: 0.8311\n",
      "Train Epoch: 3 [64000/112800 (57%)]\tLoss: 1.0807\n",
      "Train Epoch: 3 [76800/112800 (68%)]\tLoss: 1.0231\n",
      "Train Epoch: 3 [89600/112800 (79%)]\tLoss: 0.7317\n",
      "Train Epoch: 3 [102400/112800 (91%)]\tLoss: 0.8\n",
      "\n",
      "Test set: Avg. loss: 0.5708, Accuracy: 15306/18800 (81%)\n",
      "\n",
      "Train Epoch: 4 [0/112800 (0%)]\tLoss: 0.9393\n",
      "Train Epoch: 4 [12800/112800 (11%)]\tLoss: 0.8504\n",
      "Train Epoch: 4 [25600/112800 (23%)]\tLoss: 0.5697\n",
      "Train Epoch: 4 [38400/112800 (34%)]\tLoss: 0.8785\n",
      "Train Epoch: 4 [51200/112800 (45%)]\tLoss: 0.8729\n",
      "Train Epoch: 4 [64000/112800 (57%)]\tLoss: 0.9195\n",
      "Train Epoch: 4 [76800/112800 (68%)]\tLoss: 0.7056\n",
      "Train Epoch: 4 [89600/112800 (79%)]\tLoss: 0.9426\n",
      "Train Epoch: 4 [102400/112800 (91%)]\tLoss: 0.8552\n",
      "\n",
      "Test set: Avg. loss: 0.5253, Accuracy: 15562/18800 (82%)\n",
      "\n",
      "Train Epoch: 5 [0/112800 (0%)]\tLoss: 0.8409\n",
      "Train Epoch: 5 [12800/112800 (11%)]\tLoss: 0.7727\n",
      "Train Epoch: 5 [25600/112800 (23%)]\tLoss: 0.8542\n",
      "Train Epoch: 5 [38400/112800 (34%)]\tLoss: 0.5877\n",
      "Train Epoch: 5 [51200/112800 (45%)]\tLoss: 0.8647\n",
      "Train Epoch: 5 [64000/112800 (57%)]\tLoss: 0.6898\n",
      "Train Epoch: 5 [76800/112800 (68%)]\tLoss: 1.0317\n",
      "Train Epoch: 5 [89600/112800 (79%)]\tLoss: 0.7405\n",
      "Train Epoch: 5 [102400/112800 (91%)]\tLoss: 0.7429\n",
      "\n",
      "Test set: Avg. loss: 0.4977, Accuracy: 15711/18800 (83%)\n",
      "\n",
      "Train Epoch: 6 [0/112800 (0%)]\tLoss: 0.5547\n",
      "Train Epoch: 6 [12800/112800 (11%)]\tLoss: 0.5063\n",
      "Train Epoch: 6 [25600/112800 (23%)]\tLoss: 0.867\n",
      "Train Epoch: 6 [38400/112800 (34%)]\tLoss: 0.6859\n",
      "Train Epoch: 6 [51200/112800 (45%)]\tLoss: 0.5583\n",
      "Train Epoch: 6 [64000/112800 (57%)]\tLoss: 0.802\n",
      "Train Epoch: 6 [76800/112800 (68%)]\tLoss: 0.5685\n",
      "Train Epoch: 6 [89600/112800 (79%)]\tLoss: 0.5191\n",
      "Train Epoch: 6 [102400/112800 (91%)]\tLoss: 0.8005\n",
      "\n",
      "Test set: Avg. loss: 0.4759, Accuracy: 15670/18800 (83%)\n",
      "\n",
      "Train Epoch: 7 [0/112800 (0%)]\tLoss: 0.6535\n",
      "Train Epoch: 7 [12800/112800 (11%)]\tLoss: 0.5939\n",
      "Train Epoch: 7 [25600/112800 (23%)]\tLoss: 0.6481\n",
      "Train Epoch: 7 [38400/112800 (34%)]\tLoss: 1.0938\n",
      "Train Epoch: 7 [51200/112800 (45%)]\tLoss: 0.8858\n",
      "Train Epoch: 7 [64000/112800 (57%)]\tLoss: 0.3778\n",
      "Train Epoch: 7 [76800/112800 (68%)]\tLoss: 0.6997\n",
      "Train Epoch: 7 [89600/112800 (79%)]\tLoss: 0.4608\n",
      "Train Epoch: 7 [102400/112800 (91%)]\tLoss: 0.7259\n",
      "\n",
      "Test set: Avg. loss: 0.459, Accuracy: 15847/18800 (84%)\n",
      "\n",
      "Train Epoch: 8 [0/112800 (0%)]\tLoss: 0.6916\n",
      "Train Epoch: 8 [12800/112800 (11%)]\tLoss: 0.4791\n",
      "Train Epoch: 8 [25600/112800 (23%)]\tLoss: 0.8362\n",
      "Train Epoch: 8 [38400/112800 (34%)]\tLoss: 0.6496\n",
      "Train Epoch: 8 [51200/112800 (45%)]\tLoss: 0.6956\n",
      "Train Epoch: 8 [64000/112800 (57%)]\tLoss: 0.6949\n",
      "Train Epoch: 8 [76800/112800 (68%)]\tLoss: 0.5936\n",
      "Train Epoch: 8 [89600/112800 (79%)]\tLoss: 0.4948\n",
      "Train Epoch: 8 [102400/112800 (91%)]\tLoss: 0.6625\n",
      "\n",
      "Test set: Avg. loss: 0.4445, Accuracy: 15916/18800 (84%)\n",
      "\n",
      "Train Epoch: 9 [0/112800 (0%)]\tLoss: 0.8227\n",
      "Train Epoch: 9 [12800/112800 (11%)]\tLoss: 0.9489\n",
      "Train Epoch: 9 [25600/112800 (23%)]\tLoss: 0.6527\n",
      "Train Epoch: 9 [38400/112800 (34%)]\tLoss: 0.7422\n",
      "Train Epoch: 9 [51200/112800 (45%)]\tLoss: 0.5135\n",
      "Train Epoch: 9 [64000/112800 (57%)]\tLoss: 0.6565\n",
      "Train Epoch: 9 [76800/112800 (68%)]\tLoss: 0.5082\n",
      "Train Epoch: 9 [89600/112800 (79%)]\tLoss: 0.5161\n",
      "Train Epoch: 9 [102400/112800 (91%)]\tLoss: 0.7361\n",
      "\n",
      "Test set: Avg. loss: 0.4355, Accuracy: 16019/18800 (85%)\n",
      "\n",
      "Train Epoch: 10 [0/112800 (0%)]\tLoss: 0.6994\n",
      "Train Epoch: 10 [12800/112800 (11%)]\tLoss: 0.4717\n",
      "Train Epoch: 10 [25600/112800 (23%)]\tLoss: 0.5904\n",
      "Train Epoch: 10 [38400/112800 (34%)]\tLoss: 0.5085\n",
      "Train Epoch: 10 [51200/112800 (45%)]\tLoss: 0.7198\n",
      "Train Epoch: 10 [64000/112800 (57%)]\tLoss: 0.7471\n",
      "Train Epoch: 10 [76800/112800 (68%)]\tLoss: 0.6113\n",
      "Train Epoch: 10 [89600/112800 (79%)]\tLoss: 0.5004\n",
      "Train Epoch: 10 [102400/112800 (91%)]\tLoss: 0.7355\n",
      "\n",
      "Test set: Avg. loss: 0.4298, Accuracy: 15995/18800 (85%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f38b65049a532d962f7a01293d25f776c400c362ef7847f9b00f71f9ef158eeb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('py39': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
